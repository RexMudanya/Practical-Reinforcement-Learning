{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-03 05:13:56.711278: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2022-03-03 05:13:56.711351: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from IPython.display import clear_output\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# gym environment\n",
    "\n",
    "env = gym_super_mario_bros.make(\"SuperMarioBros-v0\")\n",
    "env = JoypadSpace(env, RIGHT_ONLY)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [6]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      4\u001B[0m done \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m100000\u001B[39m):\n\u001B[0;32m----> 7\u001B[0m     \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m done:\n\u001B[1;32m     10\u001B[0m         state \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mreset()\n",
      "File \u001B[0;32m~/Desktop/Projects/study/udemy/AI/PracticalReinforcementLearning/venv/lib/python3.8/site-packages/gym/core.py:240\u001B[0m, in \u001B[0;36mWrapper.render\u001B[0;34m(self, mode, **kwargs)\u001B[0m\n\u001B[1;32m    239\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrender\u001B[39m(\u001B[38;5;28mself\u001B[39m, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 240\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Projects/study/udemy/AI/PracticalReinforcementLearning/venv/lib/python3.8/site-packages/gym/core.py:240\u001B[0m, in \u001B[0;36mWrapper.render\u001B[0;34m(self, mode, **kwargs)\u001B[0m\n\u001B[1;32m    239\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrender\u001B[39m(\u001B[38;5;28mself\u001B[39m, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 240\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Projects/study/udemy/AI/PracticalReinforcementLearning/venv/lib/python3.8/site-packages/nes_py/nes_env.py:379\u001B[0m, in \u001B[0;36mNESEnv.render\u001B[0;34m(self, mode)\u001B[0m\n\u001B[1;32m    373\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mviewer \u001B[38;5;241m=\u001B[39m ImageViewer(\n\u001B[1;32m    374\u001B[0m             caption\u001B[38;5;241m=\u001B[39mcaption,\n\u001B[1;32m    375\u001B[0m             height\u001B[38;5;241m=\u001B[39mSCREEN_HEIGHT,\n\u001B[1;32m    376\u001B[0m             width\u001B[38;5;241m=\u001B[39mSCREEN_WIDTH,\n\u001B[1;32m    377\u001B[0m         )\n\u001B[1;32m    378\u001B[0m     \u001B[38;5;66;03m# show the screen on the image viewer\u001B[39;00m\n\u001B[0;32m--> 379\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mviewer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshow\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscreen\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    380\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrgb_array\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    381\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscreen\n",
      "File \u001B[0;32m~/Desktop/Projects/study/udemy/AI/PracticalReinforcementLearning/venv/lib/python3.8/site-packages/nes_py/_image_viewer.py:148\u001B[0m, in \u001B[0;36mImageViewer.show\u001B[0;34m(self, frame)\u001B[0m\n\u001B[1;32m    140\u001B[0m image \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpyglet\u001B[38;5;241m.\u001B[39mimage\u001B[38;5;241m.\u001B[39mImageData(\n\u001B[1;32m    141\u001B[0m     frame\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m],\n\u001B[1;32m    142\u001B[0m     frame\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    145\u001B[0m     pitch\u001B[38;5;241m=\u001B[39mframe\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m3\u001B[39m\n\u001B[1;32m    146\u001B[0m )\n\u001B[1;32m    147\u001B[0m \u001B[38;5;66;03m# send the image to the window\u001B[39;00m\n\u001B[0;32m--> 148\u001B[0m \u001B[43mimage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mblit\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwidth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_window\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwidth\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_window\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheight\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_window\u001B[38;5;241m.\u001B[39mflip()\n",
      "File \u001B[0;32m~/Desktop/Projects/study/udemy/AI/PracticalReinforcementLearning/venv/lib/python3.8/site-packages/pyglet/image/__init__.py:903\u001B[0m, in \u001B[0;36mImageData.blit\u001B[0;34m(self, x, y, z, width, height)\u001B[0m\n\u001B[1;32m    902\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mblit\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, y, z\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, width\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, height\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m--> 903\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_texture\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mblit(x, y, z, width, height)\n",
      "File \u001B[0;32m~/Desktop/Projects/study/udemy/AI/PracticalReinforcementLearning/venv/lib/python3.8/site-packages/pyglet/image/__init__.py:834\u001B[0m, in \u001B[0;36mImageData.get_texture\u001B[0;34m(self, rectangle, force_rectangle)\u001B[0m\n\u001B[1;32m    831\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_texture\u001B[39m(\u001B[38;5;28mself\u001B[39m, rectangle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, force_rectangle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m    832\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_current_texture \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[1;32m    833\u001B[0m             (\u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_current_texture\u001B[38;5;241m.\u001B[39m_is_rectangle \u001B[38;5;129;01mand\u001B[39;00m force_rectangle)):\n\u001B[0;32m--> 834\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_current_texture \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_texture\u001B[49m\u001B[43m(\u001B[49m\u001B[43mTexture\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrectangle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforce_rectangle\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    835\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_current_texture\n",
      "File \u001B[0;32m~/Desktop/Projects/study/udemy/AI/PracticalReinforcementLearning/venv/lib/python3.8/site-packages/pyglet/image/__init__.py:826\u001B[0m, in \u001B[0;36mImageData.create_texture\u001B[0;34m(self, cls, rectangle, force_rectangle)\u001B[0m\n\u001B[1;32m    823\u001B[0m     texture\u001B[38;5;241m.\u001B[39manchor_x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39manchor_x\n\u001B[1;32m    824\u001B[0m     texture\u001B[38;5;241m.\u001B[39manchor_y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39manchor_y\n\u001B[0;32m--> 826\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mblit_to_texture\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtexture\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtexture\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    827\u001B[0m \u001B[43m                     \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43manchor_x\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43manchor_y\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    829\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m texture\n",
      "File \u001B[0;32m~/Desktop/Projects/study/udemy/AI/PracticalReinforcementLearning/venv/lib/python3.8/site-packages/pyglet/image/__init__.py:976\u001B[0m, in \u001B[0;36mImageData.blit_to_texture\u001B[0;34m(self, target, level, x, y, z, internalformat)\u001B[0m\n\u001B[1;32m    974\u001B[0m glPushClientAttrib(GL_CLIENT_PIXEL_STORE_BIT)\n\u001B[1;32m    975\u001B[0m glPixelStorei(GL_UNPACK_ALIGNMENT, alignment)\n\u001B[0;32m--> 976\u001B[0m \u001B[43mglPixelStorei\u001B[49m\u001B[43m(\u001B[49m\u001B[43mGL_UNPACK_ROW_LENGTH\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrow_length\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    977\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_apply_region_unpack()\n\u001B[1;32m    979\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m target \u001B[38;5;241m==\u001B[39m GL_TEXTURE_3D:\n",
      "File \u001B[0;32m~/Desktop/Projects/study/udemy/AI/PracticalReinforcementLearning/venv/lib/python3.8/site-packages/pyglet/gl/lib.py:87\u001B[0m, in \u001B[0;36merrcheck\u001B[0;34m(result, func, arguments)\u001B[0m\n\u001B[1;32m     83\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mGLException\u001B[39;00m(\u001B[38;5;167;01mException\u001B[39;00m):\n\u001B[1;32m     84\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m---> 87\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21merrcheck\u001B[39m(result, func, arguments):\n\u001B[1;32m     88\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _debug_gl_trace:\n\u001B[1;32m     89\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# take random actions\n",
    "\n",
    "total_reward = 0\n",
    "done = True\n",
    "\n",
    "for step in range(100000):\n",
    "    env.render()\n",
    "\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "\n",
    "    state, reward, done, info = env.step(env.action_space.sample())\n",
    "    print(info)\n",
    "    total_reward += reward\n",
    "    clear_output(wait=True)\n",
    "\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # agent variables\n",
    "        self.state_space = state_size\n",
    "        self.action_space = action_size\n",
    "        self.memory = deque(maxlen=5000)\n",
    "        self.gamma = .8\n",
    "\n",
    "        # exploration vs exploitation\n",
    "        self.epsilon = 1\n",
    "        self.max_exploration = 1\n",
    "        self.min_epsilon = 0.01\n",
    "        self.decay_epsilon = .0001\n",
    "\n",
    "        # NN\n",
    "        self.main_network = self.build_network()\n",
    "        self.target_network = self.build_network()\n",
    "        self.update_target_network()  # set weights of main net to target network\n",
    "\n",
    "    def build_network(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(64, (4,4), strides=4, padding='same', input_shape=self.state_space))\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "        model.add(Conv2D(64, (4,4), strides=2, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "        model.add(Conv2D(64, (3, 3), strides=1, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Flatten())\n",
    "\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "\n",
    "        model.add(Dense(self.action_space, activation='linear'))\n",
    "\n",
    "        model.compile(loss='mse', optimizer=Adam())\n",
    "\n",
    "        return model\n",
    "\n",
    "    def update_target_network(self):\n",
    "        # avoid oscillation\n",
    "        self.target_network.set_weights(self.main_network.get_weights())\n",
    "\n",
    "    def act(self, state):\n",
    "\n",
    "        # epsilon greedy if eps is large take random action else take prediction using main net\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return np.random.randint(self.action_space)  # env.action_space.sample\n",
    "\n",
    "        Q_value = self.main_network.predict(state)\n",
    "\n",
    "        return np.argmax(Q_value[0])\n",
    "\n",
    "    def update_epsilon(self, episode):\n",
    "        # decays epsilon\n",
    "        self.epsilon = self.min_epsilon + (self.max_exploration - self.min_epsilon) * np.exp(-self.decay_epsilon * episode)\n",
    "\n",
    "    def train(self, batch_size):\n",
    "        minibatch = random.shuffle(self.memory, batch_size)  # minibatch from memory\n",
    "\n",
    "        # get variables from batch to find q-value\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = self.main_network.predict(state)\n",
    "\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                target[0][action] = (reward + self.gamma * np.amax(self.target_network.predict(next_state)))  # avoid oscilation\n",
    "\n",
    "            self.main_network.fit(state, target, epochs=1, verbose=0)\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "action_space = env.action_space.n\n",
    "state_space = (80, 88, 1)  # convert frame to grayscale\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def preprocess_state(state):\n",
    "    image = Image.fromarray(state)\n",
    "    image = image.resize((88, 80))\n",
    "    image = image.convert('L')\n",
    "\n",
    "    return np.array(image)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "num_episodes = 1000000\n",
    "num_timesteps = 400000\n",
    "batch_size = 64"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-03 05:39:15.194963: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-03-03 05:39:15.195001: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-03-03 05:39:15.195032: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (rex-HP-EliteBook-x360-1030-G2): /proc/driver/nvidia/version does not exist\n",
      "2022-03-03 05:39:15.195275: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-03 05:39:15.232001: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2899885000 Hz\n",
      "2022-03-03 05:39:15.232420: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x280dc50 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-03-03 05:39:15.232453: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    }
   ],
   "source": [
    "dqn = DQNAgent(state_space, action_space)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Satrting training')\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    Return = 0\n",
    "    done = False\n",
    "    time_step = 0\n",
    "\n",
    "    state = preprocess_state(env.reset())\n",
    "    state = state.reshape(-1, 80, 88, 1)\n",
    "\n",
    "    for t in range(num_timesteps):\n",
    "        env.render()\n",
    "        time_step += 1\n",
    "\n",
    "        action = dqn.act(state)\n",
    "\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        next_state = preprocess_state(next_state)\n",
    "        next_state = next_state.reshape(-1, 80, 88, 1)\n",
    "\n",
    "        dqn.store_transition(state, action, reward, next_state, done)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        Return += reward\n",
    "        print(\"Episode is {}\\tTotal time Step: {}\\tCurrent Reward: {}\\tEpsilon is: {}\".format(str(i), str(time_step), str(Return), str(dqn.epsilon)))\n",
    "\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        if len(dqn.memory) > batch_size and i > 5:\n",
    "            dqn.train(batch_size)\n",
    "\n",
    "    dqn.update_epsilon(i)\n",
    "    clear_output(wait=True)\n",
    "    dqn.update_target_network()\n",
    "\n",
    "    # save model\n",
    "\n",
    "\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}